
model_0 = Sequential([
    Conv2D(32, (3, 3), input_shape=input_shape, activation='ReLU'),
    MaxPool2D(pool_size=(2, 2)),
    Conv2D(32, (3, 3), activation='ReLU'),
    MaxPool2D(pool_size=(2, 2)),
    Flatten(),
    Dense(units=128, activation='ReLU'),
    Dropout(0.3),
    Dense(units=len(result_arr), activation='sigmoid')
])
Note: This is the original design
Info:
  Epoch 1/2 - 219s 10ms/step - loss: 0.5447 - accuracy: 0.8179 - val_loss: 0.4024 - val_accuracy: 0.8549
  Epoch 2/2 - 187s  9ms/step - loss: 0.4184 - accuracy: 0.8504 - val_loss: 0.3850 - val_accuracy: 0.8603
Testing accuracy: 86.03%
Result: Satisfactory

model_1 = Sequential([
    MaxPool2D(pool_size=(2, 2), input_shape=(input_size, input_size, 1)),
    Conv2D(64, (3, 3), activation='ReLU'),
    MaxPool2D(pool_size=(2, 2)),
    Conv2D(32, (3, 3), activation='ReLU'),
    MaxPool2D(pool_size=(2, 2)),
    Flatten(),
    Dropout(0.2),
    Dense(units=128, activation='sigmoid'),
    Dropout(0.3),
    Dense(units=len(result_arr), activation='sigmoid')
])
Info:
  Epoch 1/2 - 225s 10ms/step - loss: 0.6918 - accuracy: 0.7794 - val_loss: 0.4666 - val_accuracy: 0.8364
  Epoch 2/2 - 200s  9ms/step - loss: 0.4959 - accuracy: 0.8275 - val_loss: 0.4352 - val_accuracy: 0.8456
Testing accuracy: 84.56%
Rating: Poor, no improvement over original design

model_2 = Sequential([
    keras.Input(shape=(input_size, input_size, 1)),
    Conv2D(32, kernel_size=(3, 3), activation='relu'),
    MaxPool2D(pool_size=(2, 2)),
    Conv2D(64, kernel_size=(3, 3), activation='relu'),
    MaxPool2D(pool_size=(2, 2)),
    Flatten(),
    Dropout(0.5),
    Dense(units=len(result_arr), activation='softmax')
])
Info:
  Epoch 1/2 - 225s 10ms/step - loss: 0.5672 - accuracy: 0.8136 - val_loss: 0.4230 - val_accuracy: 0.8521
  Epoch 2/2 - 199s  9ms/step - loss: 0.4416 - accuracy: 0.8454 - val_loss: 0.4051 - val_accuracy: 0.8565
Testing accuracy: 85.65%
Result: Better, improved quickly, consider further long-term testing

model_3 = Sequential([
    keras.Input(shape=input_shape),
    Conv2D(16, 3, padding='same', activation='relu'),
    MaxPool2D(2),
    Conv2D(32, 3, padding='same', activation='relu'),
    MaxPool2D(2),
    Conv2D(64, 3, padding='same', activation='relu'),
    MaxPool2D(2),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.2),
    Dense(output_shape, activation='sigmoid')
])
Info:
  Epoch 1/2 - 229s 10ms/step - loss: 0.5125 - accuracy: 0.8253 - val_loss: 0.3987 - val_accuracy: 0.8567
  Epoch 2/2 - 197s  9ms/step - loss: 0.4023 - accuracy: 0.8546 - val_loss: 0.3741 - val_accuracy: 0.8630
Testing accuracy: 86.30%
Result: Finally, improvement over the original design!

model_4 = Sequential([
    keras.Input(shape=input_shape),
    Conv2D(16, 3, padding='same', activation='relu'),
    MaxPool2D(2),
    Conv2D(32, 3, padding='same', activation='relu'),
    MaxPool2D(2),
    Conv2D(64, 3, padding='same', activation='relu'),
    MaxPool2D(2),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(output_shape, activation='softmax')
])
Note: Small modification of prev. design: increased dropout and changed output layer activation
Info:
  Epoch 1/2 - 240s 11ms/step - loss: 0.5350 - accuracy: 0.8200 - val_loss: 0.4052 - val_accuracy: 0.8546
  Epoch 2/2 - 207s  9ms/step - loss: 0.4126 - accuracy: 0.8521 - val_loss: 0.3869 - val_accuracy: 0.8598
Testing accuracy: 85.98%
Result: Regression of performance, failure

model_5 = Sequential([
    keras.Input(shape=input_shape),
    Conv2D(16, 3, padding='same', activation='relu'),
    MaxPool2D(2),
    Conv2D(32, 3, padding='same', activation='relu'),
    MaxPool2D(2),
    Conv2D(64, 3, padding='same', activation='relu'),
    MaxPool2D(2),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(output_shape, activation='sigmoid')
])
Note: Changed output layer activation from softmax back to sigmoid to observe activation effects
Info:
  Epoch 1/2 - 242s 11ms/step - loss: 0.5277 - accuracy: 0.8214 - val_loss: 0.3941 - val_accuracy: 0.8561
  Epoch 2/2 - 206s  9ms/step - loss: 0.4094 - accuracy: 0.8530 - val_loss: 0.3791 - val_accuracy: 0.8615
Testing accuracy: 86.15%
Result: Sigmoid seems to have a notable effect on performance

model_6 = Sequential([
    keras.Input(shape=input_shape),
    Conv2D(16, 3, padding='same', activation='relu'),
    MaxPool2D(2),
    Conv2D(32, 3, padding='same', activation='relu'),
    MaxPool2D(2),
    Conv2D(64, 3, padding='same', activation='relu'),
    MaxPool2D(2),
    Conv2D(128, 3, padding='same', activation='relu'),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.3),
    Dense(128, activation='relu'),
    Dense(output_shape, activation='sigmoid')
])
Info: Added another Conv2D layer but without a MaxPool2D layer, added another Dense layer after Dropout layer
  Epoch 1/2 - 279s 13ms/step - loss: 0.5268 - accuracy: 0.8216 - val_loss: 0.3961 - val_accuracy: 0.8572
  Epoch 2/2 - 238s 11ms/step - loss: 0.4055 - accuracy: 0.8547 - val_loss: 0.3870 - val_accuracy: 0.8600
Testing accuracy: 86.00%
Result: Better, but not the best so far

model_7 = Sequential([
    keras.Input(shape=input_shape),
    Conv2D(16, 3, padding='same', activation='relu'),
    MaxPool2D(2),
    Conv2D(32, 3, padding='same', activation='relu'),
    MaxPool2D(2),
    Conv2D(64, 3, padding='same', activation='relu'),
    MaxPool2D(2),
    Conv2D(128, 3, padding='same', activation='relu'),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.2),
    Dense(128, activation='relu'),
    Dropout(0.2),
    Dense(output_shape, activation='sigmoid')
])
Info: Reduced dropout and added another Dropout layer after second dense layer
  Epoch 1/2 -
  Epoch 2/2 -
Testing accuracy:
Result:

model_8 = Sequential([
    keras.Input(shape=input_shape),
    Conv2D(16, 3, padding='same', activation='relu'),
    MaxPool2D(2),
    Conv2D(32, 3, padding='same', activation='relu'),
    MaxPool2D(2),
    Conv2D(64, 3, padding='same', activation='relu'),
    MaxPool2D(2),
    Conv2D(128, 3, padding='same', activation='relu'),
    Flatten(),
    Dense(128, activation='elu'),
    Dropout(0.2),
    Dense(128, activation='elu'),
    Dropout(0.2),
    Dense(128, activation='elu'),
    Dense(output_shape, activation='sigmoid')
])
Info: Modified Dense layer activation types
  Epoch 1/2 - 408s 19ms/step - loss: 0.4979 - accuracy: 0.8276 - val_loss: 0.4230 - val_accuracy: 0.8466
  Epoch 2/2 - 378s 17ms/step - loss: 0.4085 - accuracy: 0.8525 - val_loss: 0.4038 - val_accuracy: 0.8534
Testing accuracy: 85.34%
Result: Step time is longer, accuracy seems in a good range

model_9 = Sequential([
    keras.Input(shape=input_shape),
    Conv2D(16, 3, padding='same', activation='relu'),
    MaxPool2D(2),
    Conv2D(32, 3, padding='same', activation='relu'),
    MaxPool2D(2),
    Dropout(0.3),
    Conv2D(64, 3, padding='same', activation='relu'),
    MaxPool2D(2),
    Conv2D(128, 3, padding='same', activation='relu'),
    Flatten(),
    Dense(128, activation='elu'),
    Dense(output_shape, activation='sigmoid')
])
Info: Added dropout layer after pool #2 (recommended by "the internet")
  Epoch 1/2 - 504s 11ms/step - loss: 0.4875 - accuracy: 0.8293 - val_loss: 0.4151 - val_accuracy: 0.8460
  Epoch 2/2 - 500s 11ms/step - loss: 0.4140 - accuracy: 0.8491 - val_loss: 0.4082 - val_accuracy: 0.8499
Testing accuracy:
Result:

model_a = Sequential([
    keras.Input(shape=input_shape),
    Conv2D(16, 3, padding='same', activation='relu'),
    MaxPool2D(2),
    Conv2D(32, 3, padding='same', activation='relu'),
    MaxPool2D(2),
    Conv2D(64, 3, padding='same', activation='relu'),
    MaxPool2D(2),
    Flatten(),
    Dense(128, activation=keras.layers.PReLU()),
    Dropout(0.2),
    Dense(output_shape, activation='sigmoid')
])
Info: Using PReLu activation on Dense layer
  Epoch 1/2 - 490s 11ms/step - loss: 0.4845 - accuracy: 0.8321 - val_loss: 0.3948 - val_accuracy: 0.8574
  Epoch 2/2 - 471s 11ms/step - loss: 0.3954 - accuracy: 0.8564 - val_loss: 0.3867 - val_accuracy: 0.8592
Testing accuracy: 85.92%
Result: Great!

model_b = Sequential([
    keras.Input(shape=input_shape),
    Conv2D(16, 3, padding='same', activation=keras.layers.PReLU()),
    MaxPool2D(2),
    Conv2D(32, 3, padding='same', activation=keras.layers.PReLU()),
    MaxPool2D(2),
    Conv2D(64, 3, padding='same', activation=keras.layers.PReLU()),
    MaxPool2D(2),
    Flatten(),
    Dense(128, activation=keras.layers.PReLU()),
    Dropout(0.2),
    Dense(output_shape, activation='sigmoid')
])
Info: Testing further use of PReLu in all layers
  Epoch 1/2 - 594s 13ms/step - loss: 0.4836 - accuracy: 0.8331 - val_loss: 0.4037 - val_accuracy: 0.8559
  Epoch 2/2 - 566s 13ms/step - loss: 0.4011 - accuracy: 0.8549 - val_loss: 0.3962 - val_accuracy: 0.8572
Testing accuracy: 85.72%
Result: Did not see notable performance increase, further testing will be done on prev. model

model_c = Sequential([
    Input(shape=input_shape),
    Conv2D(16, 3, padding='same', activation='relu'),
    MaxPool2D(2),
    Conv2D(32, 3, padding='same', activation='relu'),
    MaxPool2D(2),
    Conv2D(64, 3, padding='same', activation='relu'),
    MaxPool2D(2),
    Flatten(),
    Dense(128, activation=PReLU()),
    Dropout(0.2),
    Dense(output_shape, activation='sigmoid')
])
Info: Testing model_a with epochs raised to 6

========================================================================================================================

Model #1
model = Sequential([
    Input(shape=(28, 28, 1)),
    Conv2D(32, (3, 3), activation='relu'),
    MaxPool2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPool2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPool2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dense(62, activation='sigmoid')
])
Epoch   1/100 - 406s 9ms/step - loss: 0.6524 - accuracy: 0.7859 - val_loss: 0.4686 - val_accuracy: 0.8347 - lr: 0.0050
Epoch   2/100 - 398s 9ms/step - loss: 0.4615 - accuracy: 0.8360 - val_loss: 0.4425 - val_accuracy: 0.8426 - lr: 0.0050
Epoch   3/100 - 405s 9ms/step - loss: 0.4341 - accuracy: 0.8436 - val_loss: 0.4352 - val_accuracy: 0.8435 - lr: 0.0050
Epoch   4/100 - 404s 9ms/step - loss: 0.4184 - accuracy: 0.8480 - val_loss: 0.4308 - val_accuracy: 0.8449 - lr: 0.0050
Epoch   5/100 - 404s 9ms/step - loss: 0.4082 - accuracy: 0.8509 - val_loss: 0.4304 - val_accuracy: 0.8454 - lr: 0.0050
Epoch   6/100 - 380s 9ms/step - loss: 0.4000 - accuracy: 0.8530 - val_loss: 0.4257 - val_accuracy: 0.8467 - lr: 0.0050
Epoch   7/100 - 375s 9ms/step - loss: 0.3942 - accuracy: 0.8545 - val_loss: 0.4292 - val_accuracy: 0.8459 - lr: 0.0050
Epoch   8/100 - 376s 9ms/step - loss: 0.3892 - accuracy: 0.8562 - val_loss: 0.4266 - val_accuracy: 0.8465 - lr: 0.0050
Epoch   9/100 - 375s 9ms/step - loss: 0.3851 - accuracy: 0.8568 - val_loss: 0.4318 - val_accuracy: 0.8457 - lr: 0.0050
Epoch  10/100 - 377s 9ms/step - loss: 0.3556 - accuracy: 0.8662 - val_loss: 0.4150 - val_accuracy: 0.8509 - lr: 0.0025
Epoch  11/100 - 377s 9ms/step - loss: 0.3479 - accuracy: 0.8683 - val_loss: 0.4190 - val_accuracy: 0.8513 - lr: 0.0025
Epoch  12/100 - 406s 9ms/step - loss: 0.3436 - accuracy: 0.8695 - val_loss: 0.4239 - val_accuracy: 0.8501 - lr: 0.0025
Epoch  13/100 - 456s 10ms/step - loss: 0.3404 - accuracy: 0.8706 - val_loss: 0.4256 - val_accuracy: 0.8499 - lr: 0.0025
Epoch  14/100 - 437s 10ms/step - loss: 0.3376 - accuracy: 0.8716 - val_loss: 0.4289 - val_accuracy: 0.8493 - lr: 0.0025
Epoch  15/100 - 20643s 473ms/step - loss: 0.3238 - accuracy: 0.8760 - val_loss: 0.4236 - val_accuracy: 0.8520 - lr: 0.0012
Epoch  16/100 - 9663s 222ms/step - loss: 0.3190 - accuracy: 0.8779 - val_loss: 0.4263 - val_accuracy: 0.8517 - lr: 0.0012
Epoch  17/100 - 391s 9ms/step - loss: 0.3160 - accuracy: 0.8789 - val_loss: 0.4295 - val_accuracy: 0.8517 - lr: 0.0012
Epoch  18/100 - 434s 10ms/step - loss: 0.3136 - accuracy: 0.8796 - val_loss: 0.4326 - val_accuracy: 0.8514 - lr: 0.0012
Epoch  19/100 - 439s 10ms/step - loss: 0.3077 - accuracy: 0.8814 - val_loss: 0.4278 - val_accuracy: 0.8530 - lr: 6.2500e-04
Epoch  20/100 - 408s 9ms/step - loss: 0.3047 - accuracy: 0.8823 - val_loss: 0.4305 - val_accuracy: 0.8527 - lr: 6.2500e-04
Epoch  21/100 - 399s 9ms/step - loss: 0.3027 - accuracy: 0.8831 - val_loss: 0.4330 - val_accuracy: 0.8527 - lr: 6.2500e-04
Epoch  22/100 - 414s 9ms/step - loss: 0.3011 - accuracy: 0.8836 - val_loss: 0.4353 - val_accuracy: 0.8524 - lr: 6.2500e-04
Epoch  23/100 - 406s 9ms/step - loss: 0.2986 - accuracy: 0.8843 - val_loss: 0.4313 - val_accuracy: 0.8532 - lr: 3.1250e-04
Epoch  24/100 - 402s 9ms/step - loss: 0.2967 - accuracy: 0.8849 - val_loss: 0.4323 - val_accuracy: 0.8532 - lr: 3.1250e-04
Epoch  25/100 - 377s 9ms/step - loss: 0.2955 - accuracy: 0.8853 - val_loss: 0.4336 - val_accuracy: 0.8531 - lr: 3.1250e-04
Epoch  26/100 - 378s 9ms/step - loss: 0.2944 - accuracy: 0.8858 - val_loss: 0.4347 - val_accuracy: 0.8531 - lr: 3.1250e-04
Epoch  27/100 - 377s 9ms/step - loss: 0.2935 - accuracy: 0.8861 - val_loss: 0.4314 - val_accuracy: 0.8539 - lr: 1.5625e-04
Epoch  28/100 - 379s 9ms/step - loss: 0.2924 - accuracy: 0.8865 - val_loss: 0.4319 - val_accuracy: 0.8541 - lr: 1.5625e-04
Epoch  29/100 - 377s 9ms/step - loss: 0.2916 - accuracy: 0.8868 - val_loss: 0.4325 - val_accuracy: 0.8542 - lr: 1.5625e-04
Epoch  30/100 - 378s 9ms/step - loss: 0.2910 - accuracy: 0.8870 - val_loss: 0.4333 - val_accuracy: 0.8542 - lr: 1.5625e-04
Epoch  31/100 - 382s 9ms/step - loss: 0.2904 - accuracy: 0.8872 - val_loss: 0.4339 - val_accuracy: 0.8539 - lr: 1.5625e-04
Epoch  32/100 - 380s 9ms/step - loss: 0.2901 - accuracy: 0.8872 - val_loss: 0.4327 - val_accuracy: 0.8542 - lr: 7.8125e-05
Epoch  33/100 - 377s 9ms/step - loss: 0.2894 - accuracy: 0.8875 - val_loss: 0.4329 - val_accuracy: 0.8542 - lr: 7.8125e-05
Epoch  34/100 - 378s 9ms/step - loss: 0.2890 - accuracy: 0.8876 - val_loss: 0.4332 - val_accuracy: 0.8541 - lr: 7.8125e-05
Epoch  35/100 - 379s 9ms/step - loss: 0.2889 - accuracy: 0.8876 - val_loss: 0.4324 - val_accuracy: 0.8545 - lr: 3.9062e-05
Epoch  36/100 - 378s 9ms/step - loss: 0.2885 - accuracy: 0.8878 - val_loss: 0.4326 - val_accuracy: 0.8545 - lr: 3.9062e-05
Epoch  37/100 - 378s 9ms/step - loss: 0.2882 - accuracy: 0.8879 - val_loss: 0.4327 - val_accuracy: 0.8545 - lr: 3.9062e-05
Epoch  38/100 - 381s 9ms/step - loss: 0.2880 - accuracy: 0.8880 - val_loss: 0.4329 - val_accuracy: 0.8545 - lr: 3.9062e-05
Epoch  39/100 - 385s 9ms/step - loss: 0.2879 - accuracy: 0.8879 - val_loss: 0.4324 - val_accuracy: 0.8546 - lr: 1.9531e-05
Epoch  40/100 - 380s 9ms/step - loss: 0.2877 - accuracy: 0.8881 - val_loss: 0.4324 - val_accuracy: 0.8546 - lr: 1.9531e-05
Epoch  41/100 - 378s 9ms/step - loss: 0.2875 - accuracy: 0.8881 - val_loss: 0.4325 - val_accuracy: 0.8546 - lr: 1.9531e-05
Epoch  42/100 - 378s 9ms/step - loss: 0.2874 - accuracy: 0.8882 - val_loss: 0.4326 - val_accuracy: 0.8546 - lr: 1.9531e-05
Epoch  43/100 - 379s 9ms/step - loss: 0.2873 - accuracy: 0.8882 - val_loss: 0.4324 - val_accuracy: 0.8547 - lr: 9.7656e-06
Epoch  44/100 - 382s 9ms/step - loss: 0.2872 - accuracy: 0.8883 - val_loss: 0.4324 - val_accuracy: 0.8548 - lr: 9.7656e-06
Epoch  45/100 - 412s 9ms/step - loss: 0.2872 - accuracy: 0.8883 - val_loss: 0.4324 - val_accuracy: 0.8548 - lr: 9.7656e-06
Epoch  46/100 - 451s 10ms/step - loss: 0.2871 - accuracy: 0.8884 - val_loss: 0.4325 - val_accuracy: 0.8547 - lr: 9.7656e-06
Epoch  47/100 - 442s 10ms/step - loss: 0.2870 - accuracy: 0.8884 - val_loss: 0.4324 - val_accuracy: 0.8548 - lr: 4.8828e-06
Epoch  48/100 - 442s 10ms/step - loss: 0.2869 - accuracy: 0.8884 - val_loss: 0.4324 - val_accuracy: 0.8548 - lr: 4.8828e-06
Epoch  49/100 - 447s 10ms/step - loss: 0.2869 - accuracy: 0.8884 - val_loss: 0.4324 - val_accuracy: 0.8548 - lr: 4.8828e-06
Epoch  50/100 - 446s 10ms/step - loss: 0.2869 - accuracy: 0.8885 - val_loss: 0.4324 - val_accuracy: 0.8549 - lr: 4.8828e-06
Epoch  51/100 - 446s 10ms/step - loss: 0.2868 - accuracy: 0.8885 - val_loss: 0.4324 - val_accuracy: 0.8548 - lr: 2.4414e-06
Epoch  52/100 - 446s 10ms/step - loss: 0.2868 - accuracy: 0.8885 - val_loss: 0.4324 - val_accuracy: 0.8547 - lr: 2.4414e-06
Epoch  53/100 - 439s 10ms/step - loss: 0.2868 - accuracy: 0.8885 - val_loss: 0.4324 - val_accuracy: 0.8548 - lr: 2.4414e-06
Epoch  54/100 - 429s 10ms/step - loss: 0.2867 - accuracy: 0.8885 - val_loss: 0.4324 - val_accuracy: 0.8548 - lr: 1.2207e-06
Epoch  55/100 - 439s 10ms/step - loss: 0.2867 - accuracy: 0.8885 - val_loss: 0.4324 - val_accuracy: 0.8547 - lr: 1.2207e-06
Epoch  56/100 - 441s 10ms/step - loss: 0.2867 - accuracy: 0.8885 - val_loss: 0.4324 - val_accuracy: 0.8548 - lr: 1.2207e-06
Epoch  57/100 - 444s 10ms/step - loss: 0.2867 - accuracy: 0.8885 - val_loss: 0.4324 - val_accuracy: 0.8548 - lr: 1.0000e-06
Epoch  58/100 - 432s 10ms/step - loss: 0.2867 - accuracy: 0.8885 - val_loss: 0.4324 - val_accuracy: 0.8547 - lr: 1.0000e-06
Epoch  59/100 - 437s 10ms/step - loss: 0.2867 - accuracy: 0.8885 - val_loss: 0.4324 - val_accuracy: 0.8547 - lr: 1.0000e-06
Epoch  60/100 - 426s 10ms/step - loss: 0.2867 - accuracy: 0.8885 - val_loss: 0.4324 - val_accuracy: 0.8548 - lr: 1.0000e-06
Epoch  61/100 - 425s 10ms/step - loss: 0.2867 - accuracy: 0.8885 - val_loss: 0.4324 - val_accuracy: 0.8548 - lr: 1.0000e-06
Epoch  62/100 - 431s 10ms/step - loss: 0.2867 - accuracy: 0.8885 - val_loss: 0.4324 - val_accuracy: 0.8548 - lr: 1.0000e-06
Epoch  63/100 - 432s 10ms/step - loss: 0.2867 - accuracy: 0.8885 - val_loss: 0.4324 - val_accuracy: 0.8548 - lr: 1.0000e-06
Epoch  64/100 - 446s 10ms/step - loss: 0.2867 - accuracy: 0.8885 - val_loss: 0.4324 - val_accuracy: 0.8548 - lr: 1.0000e-06
Epoch  65/100 - 442s 10ms/step - loss: 0.2867 - accuracy: 0.8885 - val_loss: 0.4324 - val_accuracy: 0.8548 - lr: 1.0000e-06
Epoch  66/100 - 448s 10ms/step - loss: 0.2867 - accuracy: 0.8885 - val_loss: 0.4324 - val_accuracy: 0.8548 - lr: 1.0000e-06
Epoch  67/100 - 431s 10ms/step - loss: 0.2866 - accuracy: 0.8885 - val_loss: 0.4324 - val_accuracy: 0.8548 - lr: 1.0000e-06
Epoch  68/100 - 431s 10ms/step - loss: 0.2866 - accuracy: 0.8885 - val_loss: 0.4324 - val_accuracy: 0.8548 - lr: 1.0000e-06
Epoch  69/100 - 432s 10ms/step - loss: 0.2866 - accuracy: 0.8885 - val_loss: 0.4324 - val_accuracy: 0.8548 - lr: 1.0000e-06
Epoch  70/100 - 432s 10ms/step - loss: 0.2866 - accuracy: 0.8885 - val_loss: 0.4324 - val_accuracy: 0.8548 - lr: 1.0000e-06
Epoch  71/100 - 439s 10ms/step - loss: 0.2866 - accuracy: 0.8885 - val_loss: 0.4325 - val_accuracy: 0.8547 - lr: 1.0000e-06
Epoch  72/100 - 429s 10ms/step - loss: 0.2866 - accuracy: 0.8885 - val_loss: 0.4325 - val_accuracy: 0.8547 - lr: 1.0000e-06
Epoch  73/100 - 445s 10ms/step - loss: 0.2866 - accuracy: 0.8885 - val_loss: 0.4325 - val_accuracy: 0.8547 - lr: 1.0000e-06
Epoch  74/100 - 442s 10ms/step - loss: 0.2866 - accuracy: 0.8885 - val_loss: 0.4325 - val_accuracy: 0.8547 - lr: 1.0000e-06
Epoch  75/100 - 434s 10ms/step - loss: 0.2866 - accuracy: 0.8885 - val_loss: 0.4325 - val_accuracy: 0.8548 - lr: 1.0000e-06
Epoch  76/100 - 434s 10ms/step - loss: 0.2866 - accuracy: 0.8885 - val_loss: 0.4325 - val_accuracy: 0.8548 - lr: 1.0000e-06
Epoch  77/100 - 395s 9ms/step - loss: 0.2866 - accuracy: 0.8885 - val_loss: 0.4325 - val_accuracy: 0.8547 - lr: 1.0000e-06
Epoch  78/100 - 406s 9ms/step - loss: 0.2866 - accuracy: 0.8885 - val_loss: 0.4325 - val_accuracy: 0.8547 - lr: 1.0000e-06
Epoch  79/100 - 396s 9ms/step - loss: 0.2866 - accuracy: 0.8885 - val_loss: 0.4325 - val_accuracy: 0.8547 - lr: 1.0000e-06
Epoch  80/100 - 399s 9ms/step - loss: 0.2866 - accuracy: 0.8885 - val_loss: 0.4325 - val_accuracy: 0.8547 - lr: 1.0000e-06
Epoch  81/100 - 402s 9ms/step - loss: 0.2866 - accuracy: 0.8885 - val_loss: 0.4325 - val_accuracy: 0.8547 - lr: 1.0000e-06
Epoch  82/100 - 396s 9ms/step - loss: 0.2866 - accuracy: 0.8885 - val_loss: 0.4325 - val_accuracy: 0.8548 - lr: 1.0000e-06
Epoch  83/100 - 392s 9ms/step - loss: 0.2866 - accuracy: 0.8885 - val_loss: 0.4325 - val_accuracy: 0.8548 - lr: 1.0000e-06
Epoch  84/100 - 388s 9ms/step - loss: 0.2866 - accuracy: 0.8885 - val_loss: 0.4325 - val_accuracy: 0.8548 - lr: 1.0000e-06
Epoch  85/100 - 430s 10ms/step - loss: 0.2865 - accuracy: 0.8885 - val_loss: 0.4325 - val_accuracy: 0.8548 - lr: 1.0000e-06
Epoch  86/100 - 444s 10ms/step - loss: 0.2865 - accuracy: 0.8885 - val_loss: 0.4325 - val_accuracy: 0.8548 - lr: 1.0000e-06
Epoch 87/100 - 585s 13ms/step - loss: 0.2865 - accuracy: 0.8886 - val_loss: 0.4325 - val_accuracy: 0.8548 - lr: 1.0000e-06
Epoch 88/100 - 534s 12ms/step - loss: 0.2865 - accuracy: 0.8885 - val_loss: 0.4325 - val_accuracy: 0.8547 - lr: 1.0000e-06
Epoch 89/100 - 634s 15ms/step - loss: 0.2865 - accuracy: 0.8886 - val_loss: 0.4325 - val_accuracy: 0.8547 - lr: 1.0000e-06
Epoch 90/100 - 437s 10ms/step - loss: 0.2865 - accuracy: 0.8886 - val_loss: 0.4325 - val_accuracy: 0.8547 - lr: 1.0000e-06
Epoch 91/100 - 471s 11ms/step - loss: 0.2865 - accuracy: 0.8886 - val_loss: 0.4326 - val_accuracy: 0.8547 - lr: 1.0000e-06
Epoch 92/100 - 480s 11ms/step - loss: 0.2865 - accuracy: 0.8886 - val_loss: 0.4326 - val_accuracy: 0.8547 - lr: 1.0000e-06
Epoch 93/100 - 488s 11ms/step - loss: 0.2865 - accuracy: 0.8886 - val_loss: 0.4326 - val_accuracy: 0.8547 - lr: 1.0000e-06
Epoch 94/100 - 484s 11ms/step - loss: 0.2865 - accuracy: 0.8886 - val_loss: 0.4326 - val_accuracy: 0.8547 - lr: 1.0000e-06
Epoch 95/100 - 479s 11ms/step - loss: 0.2865 - accuracy: 0.8886 - val_loss: 0.4326 - val_accuracy: 0.8547 - lr: 1.0000e-06
Epoch 96/100 - 479s 11ms/step - loss: 0.2865 - accuracy: 0.8886 - val_loss: 0.4326 - val_accuracy: 0.8547 - lr: 1.0000e-06
Epoch 97/100 - 481s 11ms/step - loss: 0.2865 - accuracy: 0.8886 - val_loss: 0.4326 - val_accuracy: 0.8547 - lr: 1.0000e-06
Epoch 98/100 - 481s 11ms/step - loss: 0.2865 - accuracy: 0.8886 - val_loss: 0.4326 - val_accuracy: 0.8547 - lr: 1.0000e-06
Epoch 99/100 - 763s 17ms/step - loss: 0.2865 - accuracy: 0.8886 - val_loss: 0.4326 - val_accuracy: 0.8547 - lr: 1.0000e-06
Epoch 100/100 - 484s 11ms/step - loss: 0.2865 - accuracy: 0.8886 - val_loss: 0.4326 - val_accuracy: 0.8547 - lr: 1.0000e-06

Repeated with various variables changed
model = Sequential([
    Input(shape=(28, 28, 1)),
    Conv2D(32, (3, 3), activation='relu'),
    MaxPool2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPool2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPool2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.2),
    Dense(64, activation='relu'),
    Dense(62, activation='sigmoid')
])

model = Sequential([
    Input(shape=(28, 28, 1)),
    Conv2D(32, (3, 3), activation='relu'),
    MaxPool2D((2, 2)),
    Conv2D(64, (3, 3), activation='relu'),
    MaxPool2D((2, 2)),
    Conv2D(128, (3, 3), activation='relu'),
    MaxPool2D((2, 2)),
    Flatten(),
    Dense(128, activation='relu'),
    Dense(62, activation='sigmoid')
])
Compiled with 'adam' and 'sparce_categorical_crossentropy'
 1/12 - 433s 10ms/step - loss: 0.5618 - accuracy: 0.8094 - val_loss: 0.4697 - val_accuracy: 0.8343
 2/12 - 422s 10ms/step - loss: 0.4525 - accuracy: 0.8395 - val_loss: 0.4558 - val_accuracy: 0.8382
 3/12 - 428s 10ms/step - loss: 0.4378 - accuracy: 0.8439 - val_loss: 0.4505 - val_accuracy: 0.8406
 4/12 - 431s 10ms/step - loss: 0.4317 - accuracy: 0.8455 - val_loss: 0.4535 - val_accuracy: 0.8408
 5/12 - 415s 10ms/step - loss: 0.4285 - accuracy: 0.8469 - val_loss: 0.4649 - val_accuracy: 0.8380
 6/12 - 387s  9ms/step - loss: 0.4278 - accuracy: 0.8473 - val_loss: 0.4601 - val_accuracy: 0.8410
 7/12 - 380s  9ms/step - loss: 0.4274 - accuracy: 0.8476 - val_loss: 0.4685 - val_accuracy: 0.8391
 8/12 - 390s  9ms/step - loss: 0.4281 - accuracy: 0.8474 - val_loss: 0.4744 - val_accuracy: 0.8375
 9/12 - 490s 11ms/step - loss: 0.4299 - accuracy: 0.8467 - val_loss: 0.4681 - val_accuracy: 0.8397
10/12 - 379s  9ms/step - loss: 0.4314 - accuracy: 0.8470 - val_loss: 0.4745 - val_accuracy: 0.8381
11/12 - 381s  9ms/step - loss: 0.4334 - accuracy: 0.8463 - val_loss: 0.4787 - val_accuracy: 0.8350
12/12 - 477s 11ms/step - loss: 0.4357 - accuracy: 0.8461 - val_loss: 0.4765 - val_accuracy: 0.8374
Final Results:
- loss: 0.4357
- accuracy: 0.8461

Retest with new features:
- optimizer = Adam(epsilon=0.1)
- loss = 'categorical_crossentropy'
- callbacks = ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=2, min_lr=0.0001)
Epoch  1/12 - 419s 10ms/step - loss: 0.8454 - accuracy: 0.7403 - val_loss: 0.5575 - val_accuracy: 0.8079 - lr: 0.0010
Epoch  2/12 - 380s  9ms/step - loss: 0.5155 - accuracy: 0.8210 - val_loss: 0.4906 - val_accuracy: 0.8287 - lr: 0.0010
Epoch  3/12 - 380s  9ms/step - loss: 0.4686 - accuracy: 0.8343 - val_loss: 0.4655 - val_accuracy: 0.8356 - lr: 0.0010
Epoch  4/12 - 376s  9ms/step - loss: 0.4440 - accuracy: 0.8414 - val_loss: 0.4519 - val_accuracy: 0.8391 - lr: 0.0010
Epoch  5/12 - 363s  8ms/step - loss: 0.4280 - accuracy: 0.8458 - val_loss: 0.4415 - val_accuracy: 0.8419 - lr: 0.0010
Epoch  6/12 - 365s  8ms/step - loss: 0.4163 - accuracy: 0.8493 - val_loss: 0.4337 - val_accuracy: 0.8443 - lr: 0.0010
Epoch  7/12 - 380s  9ms/step - loss: 0.4073 - accuracy: 0.8519 - val_loss: 0.4289 - val_accuracy: 0.8457 - lr: 0.0010
Epoch  8/12 - 373s  9ms/step - loss: 0.3998 - accuracy: 0.8542 - val_loss: 0.4245 - val_accuracy: 0.8473 - lr: 0.0010
Epoch  9/12 - 380s  9ms/step - loss: 0.3934 - accuracy: 0.8558 - val_loss: 0.4208 - val_accuracy: 0.8478 - lr: 0.0010
Epoch 10/12 - 379s  9ms/step - loss: 0.3880 - accuracy: 0.8572 - val_loss: 0.4188 - val_accuracy: 0.8482 - lr: 0.0010
Epoch 11/12 - 375s  9ms/step - loss: 0.3831 - accuracy: 0.8587 - val_loss: 0.4165 - val_accuracy: 0.8492 - lr: 0.0010
Epoch 12/12 - 369s  8ms/step - loss: 0.3787 - accuracy: 0.8600 - val_loss: 0.4162 - val_accuracy: 0.8494 - lr: 0.0010
